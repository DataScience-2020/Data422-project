{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangler Group Project Report\n",
    "\n",
    "==============================================================\n",
    "\n",
    "Team Members:\n",
    "\n",
    "Name: Shi Chen\n",
    "\n",
    "Student ID: 54638177\n",
    "\n",
    "Name: Jian ZHOU\n",
    "\n",
    "Student ID: 51404140\n",
    "\n",
    "Name: Jiaying Zhu\n",
    "\n",
    "Student ID: 44820888\n",
    "\n",
    "Name: Xiaohong Chen\n",
    "\n",
    "Student ID: 24908339\n",
    "\n",
    "Name: Waqas Naveed\n",
    "\n",
    "Student ID: 88354613\n",
    "\n",
    "Date of submission: 23/10/2020\n",
    "\n",
    "==============================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "Our team is hired by ABC Game development company. Our team is responsible for data collection which will help development department to make decision. \n",
    "\n",
    "##### Our tasks are divided into 3 parts:\n",
    "\n",
    "    First part is about game market information collection;\n",
    "\n",
    "    Second part is about new game storyline design;\n",
    "\n",
    "    Third part is about new game payment design."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 Game market information collection\n",
    "####  \n",
    "\n",
    "\n",
    "#### Data Source: IGDB API\n",
    "\n",
    "Scource link (IGDB API documents): https://api-docs.igdb.com/#endpoints\n",
    "\n",
    "IGDB introduction: https://www.igdb.com/about \n",
    "\n",
    "\n",
    "#### Why choose this data source?\n",
    "\n",
    "* From the view of authority: \n",
    " \n",
    " the community and developers in this flat are generous, they wish to share data.\n",
    "\n",
    "* Professional: \n",
    " \n",
    " IGDB is games ”one-stop-infospot”, which allow users to do more with the information in order to explore and find games to play in a new way. From the authority point of view. And it contains 245,500 games, more than 35,647 members and involved 27,894 companies. It is very professional API resources.\n",
    " \n",
    "* It is free!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target \n",
    "\n",
    "In this part, we are going to generate information which will benefit to analysis the popular games. \n",
    "\n",
    "* Genre (market preference)\n",
    "* Age_rating (target user)\n",
    "* Developer map (parterners or competitors)\n",
    "* Keyword pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenges\n",
    "\n",
    "* Token\n",
    "\n",
    "Sarah and Christine are responsible for this part. To new hands, challenges are everywhere happens everyday. Scapping data from API, the first thing is to obtain \"Token\" from API Database. It is the key. We Follow the process list on API document, doing it step by step, though totally have no idea what will happen next. With Gulio's help, we eventrally can post require to IGDB API with Token. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* API documents\n",
    "\n",
    "There are a lot of information in the API documents, \"authentication\" \"Example\" \"Endpoints\" contains a lot of informations about the game dataset. We did not read them carefully in the beginning, and made problem when doing the data cleaning. When we realized it, we already waste a lot of time. This is a big lesson. Whenever collectiong data, the first thing is figure out what kind of data need to be collected, it must be clear and reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Coding\n",
    "\n",
    "We using Julia to do this part. Not easy at all. From \"post\" to generate string information from web, then transform them into JSON format then output as CSV. The process seems very clear, but when you doing it, there are a lot of small tricks waiting to stuck you. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Logic\n",
    "\n",
    "This is a very important part.As mentioned before, in the very begining, we generate some data, then we think it will work. But acturally we use the wrong logic, the data cannot be used, too many noice data, missing value data. After we define what is \"popular game\", how to filter them directly rather than scapping all the games' data. Things become clear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Correlation\n",
    "\n",
    "Here, the correlation means the relationship between main dataset and sub dataset. In the main game information dataset, there are plenty encoding values cannot be used directly. This encoding values need another sub dataset to decoding. That is why we generate 10 sub dataset. And there are some features' type are list, it is another challenge part. We have not solve yet.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Github\n",
    "\n",
    "Selflearn how to use Github. It is a challenge for us in the begining, but it is quite convinient and smart after we familiar with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Techniques\n",
    "\n",
    "In this part we use Julia as tool to scrapping games' information then using .csv as output data format, there are some wrapped function been used which will make life easy.\n",
    "\n",
    "To facilitate version control and collaboration, we use Github to manage the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Achieved and failed\n",
    "\n",
    "* Achieved\n",
    "\n",
    "1. Our main target dataset (the main popular games data) was generate successfully (filename: target_games_fellow.csv). It is in the Data422-project/IGDB/Docs directory. This file contains the top 500 popular games and 25 game's features information.\n",
    "\n",
    "(Popular games selection logic please refer to Data422-project/IGDB/Codes/IGDB_games_infor_scrap_main.ipnb section 1 keyword \"body_fellow\")\n",
    "\n",
    "2. Another 10 sub datasets (decoding dataset related to these 500 popular games), which can be used to do different group analysis. \n",
    "\n",
    "Such as game_ageRating_groupCount_plot.png and game_ageRating_groupCount.csv is using the data from game_ageRating sub dataset. You can do more as you like, such as genre groupcount, releaseDates analysis, game_theme groupcount and so on.\n",
    "\n",
    "* Failed\n",
    "\n",
    "1. Companies information sub dataset was generate in wrong coding. Have tried many ways but still not been solved yet.\n",
    "\n",
    "2. The companies world mapping is using the wrong data, so please use it carefully.\n",
    "\n",
    "* Haven't figure out\n",
    "Julia technique: \n",
    "1. Convert list column into a set; \n",
    "2. Join by list column; \n",
    "3. Groupby id summarize column value as set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Source: Geo Data\n",
    "\n",
    "Scource link (countries code): https://stefangabos.github.io/world_countries/\n",
    "\n",
    "#### Why choose this data source?\n",
    "\n",
    "1. looking for a suitable geo data source to combine with the existing one subset(game company) of IGDB to draw a map.\n",
    "\n",
    "2. It is open-source.\n",
    "\n",
    "#### Achieved and failed\n",
    "\n",
    "* Achieved\n",
    "\n",
    "Geo selection please refer to Data422-project/Geo/Codes/geoData.ipynb(2.connect datasets for visualization), there is one global map to view the distribution of 21 countries and one bar chart to view the company(developer) counts in each contries.\n",
    "\n",
    "* Failed\n",
    "\n",
    "1. The companies world mapping is using the wrong data, but at least try to plot. By using R(ploty) it came to an issue for presentation in Jupyter lab.\n",
    "\n",
    "2. Since we changed topic in the middle part of project, other geo resources could not be transformed. \n",
    "\n",
    "3. Geo data API contains many limitations (such as Google Geo API, NZ Post API), so they are not be used in fianl project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 Movie information collection\n",
    "####  \n",
    "\n",
    "#### Data Source: IMDB website & Twitter API\n",
    "IMDB does not provide an API to facilitate automated queries. However, we could scrape the web pages to get the data. Twitter API is also a convenient and reliable datasource.\n",
    "\n",
    "\n",
    "#### Why choose these data sources?\n",
    "\n",
    "* IMDb is the world's most popular and authoritative source for movie, TV and celebrity content to find ratings and reviews for the newest movie and TV shows.\n",
    "\n",
    "* Twitter API to search for keywords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target \n",
    "\n",
    "We focus on using movie dataset to support game design by discovering some of the prominent storyline and characters. \n",
    "\n",
    "* Movie \n",
    "* Storyline\n",
    "* Characters\n",
    "* Locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenges\n",
    "\n",
    "* Coding\n",
    "\n",
    ">Jay is responsible for this part. We don't have an API to use for the movies and characters, and the IMDB has a very large dataset. For movies, we name, year, rating, votes, directors and cast member from two different web pages. \n",
    "\n",
    ">The Programming languages used are R (for twitter API, sf map, NLP wordcloud2), Julia (for IMDB web scraping) and Java (for Clavin storyline locations extraction).\n",
    "\n",
    "* Big Data\n",
    "\n",
    ">There are millions of records for cast members, in addition to decades’ movie and stars numerical and text data. IMDB contains around 10,000 movies for each year’s record."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Techniques\n",
    "\n",
    ">NLP analysis to extract geographic locations from movie stories, and created wordcloud for characters and storylines from both IGDB and IMDB datasets.\n",
    "\n",
    ">Incrementally save data to files.\n",
    "\n",
    ">Two steps approach: \n",
    ">>1. Movie data with IMDb ID. \n",
    ">>2. Scrape cast page based on movie table.\n",
    "\n",
    ">Multitasking process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Achieved and Issues\n",
    "\n",
    ">To avoid ethical and privacy problems, only text mining fictitious characters instead of real names.\n",
    "\n",
    ">Finally gathered 0.3 million records cast member dataset, in addition to 10 years’ movie and stars numerical and text data, around 15MB movie and stars data, as well as a 12MB cast table, not including the text files for storyline and characters, which are also big enough to do text mining. This cast table especially can be used for other data scientists for other purposes, such as to find the most valuable actors / actresses throughout the years, when combined with movie and / or stars table, which possess rating, score, voting etc.\n",
    "\n",
    ">NLP analysis to extract geographic locations from movie stories, and created wordcloud for characters and storylines from both IGDB and IMDB datasets. CLAVIN on GitHub to extract geographic information from text files, but went through a long process to install it on local computer.\n",
    "\n",
    ">FlipTextAnalysis library in R to do NLP sentiment analysis in wordcloud2.\n",
    "\n",
    ">rtweet API to get game and movie text and wrote code snippets to filter out stop words.\n",
    "\n",
    ">Simple Features (sf) R library combined with a world geojson file I found online to plot storyline locations in 2010 and 2020 and found that India, China, France, Mexico and Japan are some of the hotspots. The popularity of India continues but China decreases.\n",
    "\n",
    ">Open Source projects are sometimes not so perfect in case of Clavin producing some unnecessary data, making the output untidy, but it’s a nice attempt. The graph output still gives us some intuitive peeking into the data we managed to retrieve from both IGDB and IMDB.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 Cryptocurrency Payment\n",
    "####  \n",
    "\n",
    "#### Data Source: CoinMarketCap\n",
    "Scource link:    https://coinmarketcap.com/all/views/all/\n",
    "\n",
    "#### Why choose these data sources?\n",
    "\n",
    "The website 'CoinmarketCap' is famous in the cryptocurrency field, and it is well-structured. Therefore, I chose the CoinMarketCap platform as our data source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target \n",
    "\n",
    "Our team focus on the global markets and wants to choose digital currencies as the payment method for in-game sales rather than use fiat currencies, as there are following advantages using cryptocurrencies as the payment method. \n",
    "\n",
    "1. Traditional international payments usually involve fees and exchange costs, cryptocurrencies only have very low transaction fees as there are no intermediate agencies.\n",
    "2. The transfer of cryptocurrency happens very quickly, eliminating the inconvenience of typical authorization requirements and wait periods.\n",
    "3. Cryptocurrency market is anonymous, users won't be tracked by third party companies.\n",
    "4. Users can get some fun using cryptocurrencies to pay for the game, and they may earn money if they hold some profitable coins.\n",
    "\n",
    "I am going to select some coins for our game through explore data from the famous website 'CoinMarketCap'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenges\n",
    "\n",
    "* Coding\n",
    "\n",
    "Linda is responsible for this part. The website 'CoinMarketCap' has API for python but not for R. I wrote two functions to get historical cryptocurrency data and merge the dataframe together into one dataframe.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plotting\n",
    "\n",
    "To represent cryptocurrency trends in the last 12 months by days more clearly, I explored the gganimate to plot the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Github\n",
    "\n",
    "It is a little bit diffucult for me to learn how to use github in the beginning. After some trying and our team members' help, I figure out how to use this platform. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Techniques\n",
    "\n",
    "I used R to finish the cryptocurrency part. After scraping data from website, I saved it as a csv file, and did some wrangling for plots in the Jupyter Notebook. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Achieved and failed\n",
    "\n",
    "* Achieved\n",
    "\n",
    "1. Top 200 cryptocurrency information including coins’ name, symbol, market cap, price, circulating supply, volumn 24h, change 1h, change 24h and change 7days.\n",
    "\n",
    "2. Found ‘Tether’, ‘Bitcoin’, and ‘Ethereum’ are very popular recently by using ggplot.\n",
    "\n",
    "3. Obtained top5 cryptocurrencies data in the last 12 months and merge the dataframe together.\n",
    "\n",
    "4. Using gif plots to illustrate the data and find patterns. \n",
    "\n",
    "\n",
    "* Failed\n",
    "\n",
    "1. Web scraping 2000 cryptocurrency data at once from 'CoinMarketCap'.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
